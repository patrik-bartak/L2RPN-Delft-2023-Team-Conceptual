{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRFdYVrQ8Grv"
   },
   "source": [
    "# **L2RPN-WCCI Example Run of RL Agents**\n",
    "\n",
    "This short tutorial notebook provides a quick guidance for installing and testing some Reinforcement Learning (RL) algorithms with Grid2Op framework. The RL algorithm used in **Section-I** is taken from [l2rpn_baselines](https://github.com/rte-france/l2rpn-baselines/tree/master/l2rpn_baselines), and that used in **Section-II** is taken from [Ray-RLlib](https://docs.ray.io/en/master/rllib.html). \n",
    "\n",
    "**A quick walkthrough:**\n",
    "- Install Grid2op and l2rpn_baselines using pip command.\n",
    "- Sample codes of DeepQSimple, DuelQSimple, DuelQLeapNet, DoubleDuelingDQN, DoubleDuelingRDQN are available in l2rpn_baselines, for brevity only the usage of DeepQSimple is shown in **Section-I**.\n",
    "- Please note these codes are just used to show the implementation. The performnaces are not tuned for the given codes. The action space, observation space and neural network architecture are chosen randomly.\n",
    "- \"l2rpn_wcci_2022\" is used as the environment for this example.\n",
    "- Please note, to use expert_agent (can be found in l2rpn_baselines), one need to install [ExpertOp4Grid](https://expertop4grid.readthedocs.io/en/latest/).\n",
    "\n",
    "- In **Section-II**, install RLlib. The DQN algorithms from RLlib are implemented as examples. Here, also the performances are not tuned. Check [training API](https://docs.ray.io/en/master/rllib-training.html) for RLlib algoritms.\n",
    "- Please note, to use grid2op environment with RLlib, there is a need to tighten the gap between grid2op and OpenAI Gym environments. Hence, The observation space and action space are made compatible with gym enviorment. To learn more on this, please check [grid2op.gym_compat](https://grid2op.readthedocs.io/en/latest/gym.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lulCCvnA_XuU"
   },
   "source": [
    "# **Section-I (RL Algorithms from l2rpn_baselines)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWnRs9FU_TMq"
   },
   "outputs": [],
   "source": [
    "#!pip3 install grid2op  # for use with google colab (grid2Op is not installed by default)\n",
    "#!pip3 install l2rpn_baselines.   # for use with google colab (l2rpn_baselines is not installed by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from l2rpn_baselines.PPO_SB3 import train as ppo_train\n",
    "from l2rpn_baselines.PPO_SB3 import evaluate as ppo_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and evaluate a Proximal Policy Optimization agent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid2op.make(\"l2rpn_wcci_2022\")\n",
    "agent = ppo_train(env, name=\"PPO_SB3\", save_path=\"baseline\", iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2op_agent, res = ppo_evaluate(\n",
    "                            env,\n",
    "                            load_path=\"baseline/\",\n",
    "                            name=\"PPO_SB3\",\n",
    "                            nb_episode=10,\n",
    "                            obs_space_kwargs={},\n",
    "                            act_space_kwargs={}\n",
    "                          )\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "  msg_tmp = \"chronics at: {}\".format(chron_name)\n",
    "  msg_tmp += \"\\ttotal score: {:.6f}\".format(cum_reward)\n",
    "  msg_tmp += \"\\ttime steps: {:.0f}/{:.0f}\".format(nb_time_step, max_ts)\n",
    "  print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcBXIxMd-2co"
   },
   "source": [
    "# **Section-II** **(RL Algorithms from Ray-RLlib)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1gTyQdlTzQ1"
   },
   "source": [
    "**Installation of RLlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!$sys.executable -m pip install 'ray[rllib]' # Install RLLib\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lH_ZOq_AyNG"
   },
   "source": [
    "# **RLlib code for DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aITmevOv1zz"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "import gym\n",
    "import numpy as np\n",
    "from ray.tune.logger import pretty_print\n",
    "import shutil\n",
    "import os\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        import grid2op\n",
    "        from grid2op.gym_compat import GymEnv\n",
    "        from grid2op.gym_compat import BoxGymActSpace\n",
    "        from grid2op.Reward import L2RPNReward\n",
    "\n",
    "\n",
    "        # 1. create the grid2op environment\n",
    "        if not \"env_name\" in env_config:\n",
    "            raise RuntimeError(\"The configuration for RLLIB should provide the env name\")\n",
    "        nm_env = env_config[\"env_name\"]\n",
    "        del env_config[\"env_name\"]\n",
    "        self.env_glop = grid2op.make(nm_env, **env_config, reward_class=L2RPNReward)\n",
    "\n",
    "        # 2. create the gym environment\n",
    "        self.env_gym = GymEnv(self.env_glop)\n",
    "        obs_gym = self.env_gym.reset()\n",
    "\n",
    "        # 3. (optional) customize it (see section above for more information)\n",
    "        ## customize action space\n",
    "        self.env_gym.action_space = BoxGymActSpace(self.env_glop.action_space,\n",
    "                                                     attr_to_keep=[\"redispatch\", \"curtail\", \"set_storage\"])\n",
    "        # The possible attribute you can provide in the \"attr_to_keep\" are:\n",
    "        # - \"redispatch\"\n",
    "        # - \"set_storage\"\n",
    "        # - \"curtail\"\n",
    "        # - \"curtail_mw\" (same effect as \"curtail\")\n",
    "\n",
    "        ## customize observation space\n",
    "        ob_space = self.env_gym.observation_space\n",
    "        ob_space = ob_space.keep_only_attr([\"rho\"])\n",
    "        \n",
    "        self.env_gym.observation_space = ob_space\n",
    "\n",
    "        # 4. specific to RLlib\n",
    "        self.action_space = self.env_gym.action_space\n",
    "        self.observation_space = self.env_gym.observation_space\n",
    "        self.step_count = 0\n",
    "        self.case_no = 0\n",
    "        self.reward_sum = 0\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env_gym.reset()\n",
    "        self.case_no += 1\n",
    "        self.reward_sum = 0\n",
    "        return obs\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        obs, reward, done, info = self.env_gym.step(action)\n",
    "        self.reward_sum += reward\n",
    "        return obs, reward, done, info\n",
    "CHECKPOINT_ROOT = \"tmp/rllib\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check this link for RLlib Training API: https://docs.ray.io/en/master/rllib-training.html\n",
    "nb_step_train = 1\n",
    "\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "for n in range(nb_step_train):  # remember: don't forge to change this number to perform an actual training !\n",
    "    from ray.rllib.agents import ppo  # import the type of agents (Change accordingly for PPO / ARS / APPO / A3C / A2C)\n",
    "    # fist initialize ray\n",
    "    config = ppo.DEFAULT_CONFIG.copy()\n",
    "    config[\"timesteps_per_iteration\"] = 10\n",
    "    config[\"num_workers\"] = 1\n",
    "    ray.init()\n",
    "    try:\n",
    "        # then define a \"trainer\" (Change accordingly for PPO / ARS / APPO / A3C / A2C)\n",
    "        trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
    "            \"env_config\": {\"env_name\":\"l2rpn_wcci_2022\"},  # config to pass to env class\n",
    "        })\n",
    "        # and then train it for a given number of iteration\n",
    "        for step in range(nb_step_train):\n",
    "            result = trainer.train()\n",
    "            \n",
    "            file_name = trainer.save(CHECKPOINT_ROOT)\n",
    "\n",
    "            print(s.format(\n",
    "              n + 1,\n",
    "              result[\"episode_reward_min\"],\n",
    "              result[\"episode_reward_mean\"],\n",
    "              result[\"episode_reward_max\"],\n",
    "              result[\"episode_len_mean\"],\n",
    "              file_name\n",
    "            ))\n",
    "            #print(pretty_print(result))\n",
    "    finally:   \n",
    "        # shutdown ray\n",
    "        ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "07_L2RPN_ICAPS test and install_baselines.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ac16c9cda3e1ca3ba8ef92137292ce35515d0771bf3965bd3a22025532cdb42f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
